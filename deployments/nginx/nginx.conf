events {
    # Max simultaneous connections per Nginx worker process.
    worker_connections 1024;
}

http {

    # SECURITY/RATE LIMITING (step 1/2): Define a shared "rate limit zone"
    # - '$binary_remote_addr' = Key = client IP (= efficient/binary form)
    # - 'zone=apilimit:10m'  - this declares the named zone 'apilimit' where Nginx 
    #   stores per-client counters/state - with a 10m memory limit 
    # - 'rate=10r/s' = Base rate - 10 requests/second per IP
    # NOTE: defining the zone alone does NOT limit anything yet; we must apply it in a server/location block.    
    limit_req_zone $binary_remote_addr zone=apilimit:10m rate=10r/s;    

    # LOAD BALANCING: Define target groups for the API replicas
    #
    # - Nginx does NOT talk to "containers by name".
    # - It talks to IP:port targets.
    # - Docker Compose provides an internal DNS server for the compose network.
    # - When we scale `api-v1` to 3 replicas (via 'docker compose up..  --scale api-v1=3...'), 
    #   Docker DNS resolves `api-v1` to multiple IPs (one per replica).
    # - Nginx uses those resolved IPs as the actual backend targets and distributes requests across them.
    #
    # upstream = a named backend "pool" (a group of servers Nginx can proxy to)
    # Define upstream targets by docker-compose *service name* (Compose provides DNS for these names)
    # Both APIs listen on port 8000 inside their containers
    upstream api_v1_pool {
        # Key idea:
        # - `api-v1` is the *compose service name* (internal DNS entry on the compose network).
        # - With `--scale api-v1=3`, that DNS name points to multiple replica IPs.
        # - Nginx will send requests to those replica IPs (not always the same one).        
        server api-v1:8000;

        # Load balancing strategy:
        # - Default is round-robin (cycle through available backend addresses).
        # - least_conn: send to the backend with the fewest active connections (better under uneven load).
        # - ip_hash: sticky-ish sessions based on client IP (same client tends to hit same backend).
        # Note: for this exam, default round-robin is usually sufficient.
        # least_conn;
        # ip_hash;   
    }

    # Separate upstream for v2 (usually only 1 instance for A/B / debug)
    upstream api_v2_single {
        server api-v2:8000;
    }

    # A/B ROUTING/TESTING (step 1/2): choose upstream (and thus api-v) based on request header
    # (default -> route to api_v1_pool, if client sends 'X-Experiment-Group: debug' => route to api_v2_single)   
    #
    # map <source_var> <target_var> { <match> <value>; ... }
    #
    # Here:
    # - source_var  = $http_x_experiment_group  (the incoming request header "X-Experiment-Group")
    # - target_var  = $predict_upstream         (a NEW variable we define, used later in proxy_pass)
    #
    # How it works:
    # 1) Nginx reads $http_x_experiment_group from the request (missing header => empty => falls back to "default").
    # 2) It compares that value against the keys inside { ... } (e.g. "debug").
    # 3) If a key matches, it assigns the corresponding value to $predict_upstream.
    # 4) If nothing matches, it assigns the "default" value.
    #
    # Result:
    # - default -> $predict_upstream = api_v1_pool
    # - "debug" -> $predict_upstream = api_v2_single
    # Then proxy_pass uses it: proxy_pass http://$predict_upstream;    
    map $http_x_experiment_group $predict_upstream {
        default  api_v1_pool;
        debug    api_v2_single;
    }

    # HTTP entrypoint: redirect everything to HTTPS
    server {
        # Nginx listens on port 80 *inside the container*.
        # Docker publishes it to the host via compose `ports: - "8080:80"`.           
        listen 80;
        server_name localhost;
        return 301 https://$host$request_uri;
    }

    # HTTPS entrypoint: terminate TLS and proxy to the API pool
    server {     
        listen 443 ssl;
        server_name localhost;

        # SSL SETUP
        ssl_certificate /etc/nginx/certs/nginx.crt;
        ssl_certificate_key /etc/nginx/certs/nginx.key;
        ssl_protocols TLSv1.2 TLSv1.3;
        ssl_ciphers HIGH:!aNULL:!MD5;
        ssl_prefer_server_ciphers on;

        # Forward /predict to v1 by default - v2 will be used later for A/B testing based on request headers.
        location /predict {
            # Basic Auth: protect this endpoint with username/password
            auth_basic "API Access Protected"; # Message displayed to the user
            auth_basic_user_file /etc/nginx/.htpasswd; # Path to the .htpasswd file in the container

            # Ratelimiting
            # Apply rate limiting to this endpoint only
            # burst=5 allows short spikes; requests beyond that are rejected (503)

            # SECURITY/RATE LIMITING (step 2/2): Enforce the previously defined zone on this endpoint only
            # - Uses zone "apilimit" (defined above)
            # - burst=5 allows short spikes above 10r/s (allows 5 extra "burst" requests before 503 error)
            # - nodelay: don't throttle/slow down; once burst is exceeded -> reject with 503            
            limit_req zone=apilimit burst=5 nodelay;

            # Reverse proxy + routing: Redirect requests to the upstream group
            # - Client sends request to Nginx (/predict)
            # - Nginx selects a backend (A/B decision via `map` -> $predict_upstream)
            # - Then proxies the request to that backend upstream group (which may itself load-balance)
            #
            # A/B routing (step 2/2): 
            # - If request has header `X-Experiment-Group: debug` -> $predict_upstream = api_v2_single
            # - Otherwise -> $predict_upstream = api_v1_pool
            #
            # Load balancing happens inside the selected upstream:
            # - api_v1_pool can represent multiple api-v1 replicas, so requests can fan out across them
            # - api_v2_single is just one backend (no balancing needed)            
            proxy_pass http://$predict_upstream;            

            # Preserve + forward original request metadata for the backend (useful for logs and later TLS/redirect work):
            # - Host: what the client used (useful for debugging / virtual hosts later)
            # - X-Real-IP: the client IP as seen by Nginx
            # - X-Forwarded-For: a chain of client/proxy IPs (standard proxy header)
            # - X-Forwarded-Proto: http/https (important once TLS is added)
            proxy_set_header Host $host;
            proxy_set_header X-Real-IP $remote_addr;
            proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
            proxy_set_header X-Forwarded-Proto $scheme;
        }
    }

    # INTERNAL METRICS ENDPOINT (NOT published to host)
    # - separate listener on internal port (8081) to avoid the HTTP->HTTPS redirect loop for the exporter
    # - allow only loopback + Docker private ranges (portable across Compose subnets)
    #
    # Why: exporter scraping should NOT be forced through HTTP->HTTPS redirect (port 80) and NOT deal with self-signed TLS.
    # Trying to keep '/nginx_status' on the main server block (80/443) can work, but is fragile with redirects + self-signed TLS.
    # Instead we listen here on a dedicated internal port (8081). Only containers on the Compose network can reach it.
    #
    # Verify:
    # - from host:    curl -s http://localhost:9113/metrics | head
    # - from exporter: docker compose -p mlops-exam exec nginx_exporter wget -qO- http://nginx:8081/nginx_status    
    server {
        listen 8081;

        # "nginx" is the docker-compose *service name* -> Compose DNS resolves it inside the network.
        server_name nginx;

        location /nginx_status {
            stub_status on;     # exposes active connections / accepts / handled / requests
            access_log off;     # don't spam logs with scrape traffic

            # Allow only loopback + private LAN ranges commonly used by Docker networks (portable across different Compose subnets)
            # - 127.0.0.1: loopback inside the Nginx container / requests originating from inside the Nginx container itself
            # - 172.16.0.0/12 and 192.168.0.0/16: private ranges that cover typical Docker bridge subnets across machines
            #   (portable: avoids hardcoding one specific subnet like 172.19.0.0/16)
            # - deny all: Everything else is blocked (metrics must not be publicly accessible).            
            allow 127.0.0.1;
            allow 172.16.0.0/12;
            allow 192.168.0.0/16;
            deny all;
        }
    }

}

